{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lsi(data, num_topics):         \n",
    "    dictionary = corpora.Dictionary(data)    \n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]    \n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)       \n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics) \n",
    "    \n",
    "    return dictionary, corpus, lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar_documents(query, indexer):\n",
    "    # transform corpus to LSI space and index it\n",
    "    index = similarities.MatrixSimilarity(indexer)\n",
    "    \n",
    "    # Perform a similarity query against the corpus\n",
    "    sims = index[query]    \n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])    \n",
    "    \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_location(dictionary, corpus, lsi, output_path, rank_sizes): \n",
    "    queries = []\n",
    "    queries.append(['state', 'diagram'])\n",
    "    queries.append(['activity', 'diagram'])\n",
    "    queries.append(['use', 'case', 'diagram'])\n",
    "    queries.append(['collaboration', 'diagram'])\n",
    "    queries.append(['deployment', 'diagram'])\n",
    "    queries.append(['sequence', 'diagram'])\n",
    "    queries.append(['cognitive', 'support'])\n",
    "    queries.append(['logging'])\n",
    "\n",
    "    for q in queries:\n",
    "        bow_vector = dictionary.doc2bow(q)\n",
    "\n",
    "        # convert the query to LSI space\n",
    "        vec_lsi = lsi[bow_vector]    \n",
    "\n",
    "        lsi_space = lsi[corpus]\n",
    "\n",
    "        sims_rank = get_most_similar_documents(vec_lsi, lsi_space)                               \n",
    "                                \n",
    "        name_result = name_converter['_'.join(q)] + '.txt'\n",
    "        \n",
    "        for size in rank_sizes:                    \n",
    "            dir_path = os.path.join(output_path, str(size), name_result)\n",
    "            print(\"Processing: R: \", size)\n",
    "            \n",
    "            rank_size = size\n",
    "            if size == 0:\n",
    "                rank_size = len(sims_rank)\n",
    "\n",
    "            f = open(dir_path, 'w')\n",
    "            for doc in sims_rank[:rank_size]:            \n",
    "                el = doc[0]        \n",
    "                entity = docLabels[el].replace('.txt', '')              \n",
    "\n",
    "                # Remove Refinement of Inner Methods\n",
    "                if '$' in entity:                    \n",
    "                    continue                    \n",
    "                  \n",
    "                # Replace because the naming file restrictions\n",
    "                entity = entity.replace('{', '<')\n",
    "                entity = entity.replace('}', '>')\n",
    "\n",
    "                # Method Result\n",
    "                if '(' in entity:            \n",
    "                    write_form = entity.rsplit('.', 1)\n",
    "                    method = write_form[0]\n",
    "                    method += ' ' + write_form[1]       \n",
    "                    f.write(method + '\\n')\n",
    "                # Class Result\n",
    "                else:    \n",
    "                    f.write(entity + '\\n')\n",
    "            f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_converter = {}\n",
    "name_converter['state_diagram'] = 'STATEDIAGRAM'\n",
    "name_converter['activity_diagram'] = 'ACTIVITYDIAGRAM'\n",
    "name_converter['use_case_diagram'] = 'USECASEDIAGRAM'\n",
    "name_converter['collaboration_diagram'] = 'COLLABORATIONDIAGRAM'\n",
    "name_converter['deployment_diagram'] = 'DEPLOYMENTDIAGRAM'\n",
    "name_converter['sequence_diagram'] = 'SEQUENCEDIAGRAM'\n",
    "name_converter['cognitive_support'] = 'COGNITIVE'\n",
    "name_converter['logging'] = 'LOGGING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories_to_process = []\n",
    "directories_to_process.append('RandomConfig00001')\n",
    "directories_to_process.append('RandomConfig00002')\n",
    "directories_to_process.append('RandomConfig00003')\n",
    "directories_to_process.append('RandomConfig00004')\n",
    "directories_to_process.append('RandomConfig00005')\n",
    "directories_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics_comb = [100,200,300,400,500]\n",
    "rank_size_comb = [10, 100, 1000, 0] #0 == Full rank\n",
    "\n",
    "for config in directories_to_process:\n",
    "    print(\"Processing:\", config)\n",
    "    text_path = os.path.join(config, 'TEXT')\n",
    "    docLabels = [] \n",
    "    docLabels = [f for f in os.listdir(text_path) if f.endswith('.txt')]\n",
    "    data = []\n",
    "    \n",
    "    for doc in docLabels:\n",
    "        full_path = os.path.join(text_path, doc)\n",
    "        f = open(full_path, 'r')\n",
    "        content = f.read()\n",
    "        data.append(content)\n",
    "        \n",
    "    corp = [d.split() for d in data]  \n",
    "    \n",
    "    for n_comb in num_topics_comb:    \n",
    "        print(\"Processing: N: \", n_comb)\n",
    "        dictionary, corpus, lsi = build_lsi(corp, n_comb)\n",
    "        out_path = os.path.join(config, 'RESULTS', 'LSI', str(n_comb)) \n",
    "        feature_location(dictionary, corpus, lsi, out_path, rank_size_comb)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
