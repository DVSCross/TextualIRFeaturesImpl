{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import os\n",
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda(data, num_topics):\n",
    "    \n",
    "    dictionary = corpora.Dictionary(data)    \n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    lda = LdaModel(corpus=corpus,\n",
    "                   num_topics=num_topics, \n",
    "                   id2word=dictionary,\n",
    "                   alpha=1e-2,\n",
    "                   eta=0.5e-2, \n",
    "                   chunksize=300,\n",
    "                   minimum_probability=0.0,\n",
    "                   passes=2)\n",
    "    \n",
    "    return dictionary, corpus, lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jensen-Shannon Divergence\n",
    "def jensen_shannon(query, matrix):\n",
    "    P = query[None,:].T \n",
    "    Q = matrix.T\n",
    "    M = 0.5 * (P + Q)\n",
    "    return np.sqrt(0.5 * (entropy(P,M) + entropy(Q,M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar_documents(query,matrix, rank_size):    \n",
    "    sims = jensen_shannon(query,matrix)\n",
    "    return sims.argsort()[:rank_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_location(dictionary, corpus, lda, output_path, rank_sizes):  \n",
    "    queries = []\n",
    "    queries.append(['state', 'diagram'])\n",
    "    queries.append(['activity', 'diagram'])\n",
    "    queries.append(['use', 'case', 'diagram'])\n",
    "    queries.append(['collaboration', 'diagram'])\n",
    "    queries.append(['deployment', 'diagram'])\n",
    "    queries.append(['sequence', 'diagram'])\n",
    "    queries.append(['cognitive', 'support'])\n",
    "    queries.append(['logging'])\n",
    "    \n",
    "    for q in queries:        \n",
    "        bow_vector = dictionary.doc2bow(q)\n",
    "        \n",
    "        # Creating topic distribution for the query\n",
    "        new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow_vector)])\n",
    "        doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "        \n",
    "        most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist, doc_topic_dist.shape[0])\n",
    "    \n",
    "        name_result = name_converter['_'.join(q)] + '.txt'\n",
    "            \n",
    "        for size in rank_sizes:                    \n",
    "            dir_path = os.path.join(output_path, str(size), name_result)\n",
    "            print(\"Processing: R: \", size)\n",
    "            \n",
    "            rank_size = size\n",
    "            if size == 0:\n",
    "                rank_size = doc_topic_dist.shape[0]\n",
    "        \n",
    "            f = open(dir_path, 'w')\n",
    "            for el in most_sim_ids[:rank_size]:\n",
    "                entity = docLabels[el].replace('.txt', '')\n",
    "                \n",
    "                # Remove Inner Methods\n",
    "                if '$' in entity:                    \n",
    "                    continue\n",
    "                    \n",
    "                # Replace because the naming file restrictions\n",
    "                entity = entity.replace('{', '<')\n",
    "                entity = entity.replace('}', '>')\n",
    "                \n",
    "                # Method result        \n",
    "                if '(' in entity:            \n",
    "                    write_form = entity.rsplit('.', 1)\n",
    "                    method = write_form[0]\n",
    "                    method += ' ' + write_form[1]       \n",
    "                    f.write(method + '\\n')\n",
    "                # Class Result\n",
    "                else:    \n",
    "                    f.write(entity + '\\n') \n",
    "            f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_converter = {}\n",
    "name_converter['state_diagram'] = 'STATEDIAGRAM'\n",
    "name_converter['activity_diagram'] = 'ACTIVITYDIAGRAM'\n",
    "name_converter['use_case_diagram'] = 'USECASEDIAGRAM'\n",
    "name_converter['collaboration_diagram'] = 'COLLABORATIONDIAGRAM'\n",
    "name_converter['deployment_diagram'] = 'DEPLOYMENTDIAGRAM'\n",
    "name_converter['sequence_diagram'] = 'SEQUENCEDIAGRAM'\n",
    "name_converter['cognitive_support'] = 'COGNITIVE'\n",
    "name_converter['logging'] = 'LOGGING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories_to_process = []\n",
    "directories_to_process.append('RandomConfig00001')\n",
    "directories_to_process.append('RandomConfig00002')\n",
    "directories_to_process.append('RandomConfig00003')\n",
    "directories_to_process.append('RandomConfig00004')\n",
    "directories_to_process.append('RandomConfig00005')\n",
    "directories_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics_comb = [100,200,300,400,500]\n",
    "rank_size_comb = [10, 100, 1000, 0] #0 == Full rank\n",
    "\n",
    "for config in directories_to_process:\n",
    "    text_path = os.path.join(config, 'TEXT')\n",
    "    docLabels = [] \n",
    "    docLabels = [f for f in os.listdir(text_path) if f.endswith('.txt')]\n",
    "    data = []\n",
    "    \n",
    "    for doc in docLabels:\n",
    "        full_path = os.path.join(text_path, doc)\n",
    "        f = open(full_path, 'r')\n",
    "        content = f.read()\n",
    "        data.append(content)\n",
    "        \n",
    "    corp = [d.split() for d in data]  \n",
    "    \n",
    "    for n_comb in num_topics_comb:\n",
    "        print(\"Processing: N: \", n_comb)\n",
    "        dictionary, corpus, lda = train_lda(corp, n_comb)        \n",
    "        out_path = os.path.join(config, 'RESULTS', 'LDA', str(n_comb))                         \n",
    "        feature_location(dictionary, corpus, lda, out_path, rank_size_comb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
